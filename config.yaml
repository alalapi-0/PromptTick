# === PromptTick 基础配置（Round 1） ===

# 扫描节奏（下一轮才用到，这里先占位）
interval_seconds: 300           # 每轮间隔秒数，示例值：5分钟
batch_size: 1                   # 每轮处理文件个数上限（占位）

# 文件过滤与排序
file_extensions: [".txt", ".md"]  # 仅处理这些后缀（占位）
ordering: "name"                 # name|mtime（占位）

# 目录
input_dir: "in_prompts"
output_dir: "out"
log_dir: "logs"
state_path: "state.json"

# 适配器选择
adapter: "echo_adapter"          # 可切换为 generic_http_adapter

# 日志级别（INFO/DEBUG）
log_level: "INFO"

# 预留的外部 API 区域（后续轮次使用）
openai:
  model: "gpt-4.1-mini"
  temperature: 0.7
  max_output_tokens: 800
  system_prompt: null         # 可选系统提示词，null 表示不使用
  extra_headers: {}           # 追加请求头，可留空；值会自动转为字符串
  max_attempts: 3             # 限流/5xx 时的最大重试次数
  base_backoff: 1.0           # 指数退避的初始等待秒数

generic_http:
  url: ""              # e.g. https://api.example.com/generate
  method: "POST"
  timeout: 60
  headers: {}          # 支持 ${ENV:KEY} 占位；示例：Authorization: "Bearer ${ENV:API_TOKEN}"
  body_template: ""    # 示例：{"prompt": "${PROMPT}"}
  response_json_pointer: ""  # 示例：/data/text
  retries:
    max_attempts: 3
    backoff_seconds: 1.0
    retry_on_status: [429, 500, 502, 503, 504]

# 本地模型适配器占位（仅命令行桥接，无模型）
local:
  engine: "cmd"               # "cmd" | "ollama" | "custom"
  model: "llama3:instruct"    # 用于模板替换；engine=ollama 时常用
  timeout_seconds: 120        # 子进程超时，单位秒
  workdir: ""                 # 可选：子进程工作目录，默认项目根目录

  env:                        # 注入到子进程的环境变量，可用 ${ENV:VAR} 继承系统值
    HF_HOME: "${ENV:HF_HOME}"

  # 命令模板占位符：${PROMPT_PATH} / ${MODEL} / ${ARGS} / ${OUT_PATH}
  command_template: >
    ollama run ${MODEL} -p "$(cat ${PROMPT_PATH})"

  # command_template: >
  #   python scripts/fake_local_model.py --in ${PROMPT_PATH} --out ${OUT_PATH}

  args: []                    # 可选的额外参数列表，最终拼接到模板后
  output_mode: "stdout"       # "stdout" 或 "file"
  out_suffix: ".out.txt"      # output_mode=file 时，输出文件的后缀名
